---
title: "ClusteringML2"
author: "Group 17 - Thomas Trankle, Jaida Clarke, Andrew Tremblay and Jill van den Dungen"
date: "3/22/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
knit: (function(inputFile, encoding) { 
          rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file='index.html') })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Objectives: What problem are we trying to solve?
2. Dataset: Is this a real dataset? simulated dataset? Briefly explain the X's and Y's
3. Explore the dataset: Key highlights - Correlations? summary statistics? Plots?
4. Feature engineering: Do you have to transform variables? log/scale/polynomials?
5. Train/test the model: Emphasize the new library/command you're introducing here, and key tuning parameters (e.g., lambda)
6. Evaluate the model: What metrics are you using? Are you tuning any parameters? Plots?

## K-Means {.tabset}

### Create a Simulated Dataset

We begin with a simple simulated example in which there are **truly two clusters** in the
data; the first 25 observations have a mean shift relative to the next 25
observations. This makes sure we have to distinct clusters in our data.

```{r}
set.seed(2)
x <- matrix(rnorm (50*2), ncol=2) # Create a 50 X 2 matrix with samples from a 
                               # normal distribution

x[1:25,1] <- x[1:25,1]+3   # ask?>???
x[1:25,2] <- x[1:25,2]-4
```

```{r}
xmean <- matrix(rnorm(4, sd = 4),4,2)
whichOnes <- sample(1:2,50, replace = T)
x + xmean[whichOnes]
```

To perform *K-Means* clustering we use the function kmeans() (built-in to *stats* 
package)

We now perform K-means clustering with K = 2, and call the argument km.out$cluster
to display the cluster label for each observation.


```{r}
km.out <- kmeans (x,2, nstart =20) # kmeans(x, centers, iter.max = 10, nstart = 1,
                                   # algorithm = c("Hartigan-Wong", "Lloyd", "Forgy",
km.out$cluster                     # "MacQueen"), trace=FALSE)

```


The K-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to kmeans(). We
can plot the data, with each observation colored according to its cluster
assignment.

```{r}
plot(x, col=(km.out$cluster +1), 
  main="K-Means Clustering Results with K=2", 
  xlab="", ylab="", pch=20, cex=2)
```


Here the observations can be easily plotted because they are **two-dimensional**.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.

In this example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with K = 3.

```{r}
set.seed(4)
km.out <- kmeans (x,3, nstart = 20)
km.out
```

When K = 3, K-means clustering splits up the two clusters.
To run the kmeans() function in R with multiple initial cluster assignments, we use the *nstart* argument. If a value of *nstart* greater than one
is used, then K-means clustering will be performed using multiple random
assignments in Step 1 of Algorithm 10.1, and the kmeans() function will
report only the best results. Here we compare using nstart=1 to nstart=20.


Number of times to run clustering, THIS IS DPENDENT ON OUR DIST

```{r}
set.seed(3)
km.out <- kmeans (x,3, nstart =1) # nstart: if centers is a number how 
                                  #         many random sets should be chosen?
km.out$tot.withinss
```

```{r}
km.out <- kmeans (x,3, nstart =20)
km.out$tot.withinss
```

Note that km.out$tot.withinss is the total within-cluster sum of squares,
which we seek to minimize by performing K-means clustering (Equation
10.11). The individual within-cluster sum-of-squares are contained in the
vector km.out$withinss.
We strongly recommend always running K-means clustering with a large
value of nstart, such as 20 or 50, since otherwise an undesirable local
optimum may be obtained.
When performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the
set.seed() function. This way, the initial cluster assignments in Step 1 can
be replicated, and the K-means output will be fully reproducible.


## Hierarchical Clustering {.tabset}
The hclust() function implements hierarchical clustering in R. In the fol- hclust() lowing example we use the data from Section 10.5.1 to plot the hierarchical clustering dendrogram using complete, single, and average linkage clustering, with Euclidean distance as the dissimilarity measure. We begin by clustering observations using complete linkage. The dist() function is used
dist() to compute the 50 Ã— 50 inter-observation Euclidean distance matrix.

```{r}
hc.complete <- hclust(dist(x), method="complete") # tells type of cluster method, type of distance used, and total data points
```

We could just as easily perform hierarchical clustering with average or single linkage instead:

```{r}
hc.average <- hclust(dist(x), method ="average")
hc.single <- hclust(dist(x), method ="single")
```

We can now plot the dendrograms obtained using the usual plot() function.The numbers at the bottom of the plot identify each observation.

```{r pressure, echo=FALSE}
par(mfrow=c(1,3))
plot(hc.complete ,main="Complete Linkage ", xlab="", sub="",
cex=.9) # produces a cleaner plot that is easy to follow

plot(hc.average , main="Average Linkage", xlab="", sub="",
cex=.9)

plot(hc.single , main="Single Linkage", xlab="", sub="",
cex=.9) # produces branches for every single observation. Not the most comprehensive heirachrical plotting method
```

To determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the cutree() function:

```{r, echo=FALSE}
cutree(hc.complete , 2) # Shows complete separation of clusters classes
```

```{r}
cutree(hc.average , 2)
```

```{r}
cutree(hc.single , 2)
```

For this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage identifies one point as belonging to its own cluster. A more sensible answer is obtained when four clusters are selected, although there are still two singletons.


```{r, echo=FALSE}
cutree(hc.single , 4)
```

To scale the variables before performing hierarchical clustering of the observations, we use the scale() function:

```{r, echo=FALSE}
xsc=scale(x)
plot(hclust(dist(xsc), method ="complete"), main=" Hierarchical
Clustering with Scaled Features ")
```

Correlation-based distance can be computed using the as.dist() funcas.dist() tion, which converts an arbitrary square symmetric matrix into a form that the hclust() function recognizes as a distance matrix. However, this only makes sense for data with at least three features since the absolute correlation between any two observations with measurements on two features is always 1. Hence, we will cluster a three-dimensional data set

```{r, echo=FALSE}
x=matrix(rnorm (30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method ="complete"), main=" Complete Linkage
with Correlation -Based Distance", xlab="", sub ="")
```


## Applied Exercise No. 9 {.tabset}

