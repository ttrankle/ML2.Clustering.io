cut(Wage$age, 2)
library(ISLR)
data(Wage)
library(ISLR)
data(Wage)
cv.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(acceleration ~ bs(weight, degree = i), data = Auto)
cv.degree[i] <- cv.glm(Auto, regression.spline)$delta[1]
}
plot(error.vector.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(error.vector.degree, type = 'b',lty = 1)
stdev <- sd(error.vector.degree[1:9])
abline(h= min(error.vector.degree) + stdev, col = "red", lty = "dashed"
)
cv.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(acceleration ~ bs(weight, degree = i), data = Auto)
cv.degree[i] <- cv.glm(Auto, regression.spline)$delta[1]
}
plot(error.vector.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(error.vector.degree, type = 'b',lty = 1)
stdev <- sd(error.vector.degree[1:9])
abline(h= min(error.vector.degree) + stdev, col = "red", lty = "dashed")
cv.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(acceleration ~ bs(weight, degree = i), data = Auto)
cv.degree[i] <- cv.glm(Auto, regression.spline)$delta[1]
}
plot(cv.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(cv.degree, type = 'b',lty = 1)
stdev <- sd(cv.degree[1:10])
abline(h= min(cv.degree) + stdev, col = "red", lty = "dashed")
poly <- glm(wage ~ bs(jobclass + maritl, degree = i), data = Wage)
## POLY
cv.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(wage ~ bs(jobclass + maritl, degree = i), data = Wage)
cv.degree[i] <- cv.glm(Auto, regression.spline)$delta[1]
}
plot(cv.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(cv.degree, type = 'b',lty = 1)
stdev <- sd(cv.degree[1:10])
cv.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(wage ~ bs(age, degree = i), data = Wage)
cv.degree[i] <- cv.glm(Auto, regression.spline)$delta[1]
}
plot(cv.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(cv.degree, type = 'b',lty = 1)
stdev <- sd(cv.degree[1:10])
abline(h= min(cv.degree) + stdev, col = "red", lty = "dashed")
cv.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(wage ~ bs(age, degree = i), data = Wage)
cv.degree[i] <- cv.glm(Wage, poly)$delta[2]
}
plot(cv.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(cv.degree, type = 'b',lty = 1)
stdev <- sd(cv.degree[1:10])
abline(h= min(cv.degree) + stdev, col = "red", lty = "dashed")
cv.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(wage ~ bs(age, degree = i), data = Wage)
cv.degree[i] <- cv.glm(Wage, poly, K=10)$delta[2]
}
plot(cv.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(cv.degree, type = 'b',lty = 1)
stdev <- sd(cv.degree[1:10])
abline(h= min(cv.degree) + stdev, col = "red", lty = "dashed")
all.deltas = rep(NA, 10)
set.seed(111)
for (i in 1:10) {
glm.fit = glm(wage~poly(age, i), data=Wage)
all.deltas[i] = cv.glm(Wage, glm.fit, K=10)$delta[2]
}
plot(1:10, all.deltas, xlab="Degree", ylab="CV error", type="l", pch=20, lwd=2, ylim=c(1590, 1700))
min.point = min(all.deltas)
sd.points = sd(all.deltas)
abline(h=min.point + 0.2 * sd.points, col="red", lty="dashed")
abline(h=min.point - 0.2 * sd.points, col="red", lty="dashed")
legend("topright", "0.2-standard deviation lines", lty="dashed", col="red")
cv.degree <- rep(0,10)
set.seed(111)
anova_check <- c()
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(wage ~ bs(age, degree = i), data = Wage)
cv.degree[i] <- cv.glm(Wage, poly, K=10)$delta[2]
aanova_check[i] <- poly
}
plot(cv.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(cv.degree, type = 'b',lty = 1)
stdev <- sd(cv.degree[1:10])
abline(h= min(cv.degree) + stdev, col = "red", lty = "dashed")
cv.degree <- rep(0,10)
set.seed(111)
anova_check <- c()
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(wage ~ bs(age, degree = i), data = Wage)
cv.degree[i] <- cv.glm(Wage, poly, K=10)$delta[2]
anova_check[i] <- poly
}
plot(cv.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(cv.degree, type = 'b',lty = 1)
stdev <- sd(cv.degree[1:10])
abline(h= min(cv.degree) + stdev, col = "red", lty = "dashed")
anova(anova_check)
anova(unlist(anova_check))
anova_check
poly$model
poly$formula
all.deltas = rep(NA, 10)
set.seed(111)
anova_check <- c()
for (i in 1:10) {
glm.fit = glm(wage~poly(age, i), data=Wage)
all.deltas[i] = cv.glm(Wage, glm.fit, K=10)$delta[2]
anova_check[i] <- glm.fit
}
View(anova_check)
cv.degree <- rep(0,10)
set.seed(111)
anova_check <- list()
# First determine appropriate polynomial degree
for (i in 1:10 ){
poly <- glm(wage ~ bs(age, degree = i), data = Wage)
cv.degree[i] <- cv.glm(Wage, poly, K=10)$delta[2]
anova_check[i] <- poly
}
View(poly)
# Plotting polynomial Error
poly <- glm(wage ~ bs(age, degree = 3), data = Wage)
plot(poly)
plot(wage ~ bs(age, degree = 3), data = Wage)
plot(wage ~ age, data = Wage)
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])
ply.pred <- predict(poly, Wage)
ply.pred <- predict(poly, age.grid)
poly.pred <- predict(poly, age.grid)
age.grid = seq(from=agelims[1], to=agelims[2])
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])
poly.pred <- predict(poly, age.grid)
poly.pred <- predict(poly, newdata = data.frame(age.grid))
lines(age.grid, )
lines(age.grid, poly.pred, col = 'blue')
?predict
poly.pred <- predict(poly, newdata = data.frame(age = age.grid))
lines(age.grid, poly.pred, col = 'blue')
# Plotting polynomial Error
poly <- glm(wage ~ bs(age, degree = 3), data = Wage)
plot(wage ~ age, data = Wage)
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])
poly.pred <- predict(poly, newdata = data.frame(age = age.grid))
lines(age.grid, poly.pred, col = 'blue')
all.cvs = rep(NA, 10)
for (i in 2:10) {
Wage$age.cut = cut(Wage$age, i)
lm.fit = glm(wage~age.cut, data=Wage)
all.cvs[i] = cv.glm(Wage, lm.fit, K=10)$delta[2]
}
plot(2:10, all.cvs[-1], xlab="Number of cuts", ylab="CV error", type="l", pch=20, lwd=2)
# choose 8 cuts
Wage$age.cut = cut(Wage$age, 8)
stepfunc <- glm(wage~age.cut, data=Wage)
plot(wage ~ age, data = Wage)
step.pred <- predict(stepfunc, newdata = data.frame(age = age.grid))
lines(age.grid, step.pred, col = 'green')
# choose 8 cuts
Wage$age.cut = cut(Wage$age, 8)
stepfunc <- glm(wage~age.cut, data=Wage)
plot(wage ~ age, data = Wage)
step.pred <- predict(stepfunc, newdata = data.frame(age = age.grid))
# choose 8 cuts
stepfunc <- glm(wage~ cut(age,8), data=Wage)
plot(wage ~ age, data = Wage)
step.pred <- predict(stepfunc, newdata = data.frame(age = age.grid))
lines(age.grid, step.pred, col = 'green')
plot(wage ~ age, data = Wage, col='dargrey')
plot(wage ~ age, data = Wage, col='darkgrey')
step.pred <- predict(stepfunc, newdata = data.frame(age = age.grid))
lines(age.grid, step.pred, col = 'green')
lines(age.grid, step.pred, col = 'purple')
library(splines)
?ns
6-1-1
###################################################
####        Model B: Natural Spline            ####
###################################################
set.seed(5082)
library(splines)
attach(OJ) #OJ dataframe from ISLR
OJ$Purchase <- as.factor(OJ$Purchase)
x=model.matrix(Purchase~.,OJ)[,-1]
y=OJ$Purchase
#(0) Modify the code so the training set is 75% and test set is 25% of the dataframe
set.seed(5082)
train = sample(1:nrow(x), nrow(x)*.75)
test = (-train)
x.train = x[train,]
x.test = x[-train, ]
y.train = y[train]
y.test =y[-train]
needed <- c('glmnet', 'randomForest', 'splines', 'boot', 'ISLR')
installIfAbsentAndLoad(needed)
# Create dataframe
attach(OJ) #OJ dataframe from ISLR
OJ$Purchase <- as.factor(OJ$Purchase)
x=model.matrix(Purchase~.,OJ)[,-1]
y=OJ$Purchase
#(0) Modify the code so the training set is 75% and test set is 25% of the dataframe
set.seed(5082)
train = sample(1:nrow(x), nrow(x)*.75)
test = (-train)
x.train = x[train,]
x.test = x[-train, ]
y.train = y[train]
y.test =y[-train]
rm(list=ls())
####################################################
#####                Functions                 #####
####################################################
installIfAbsentAndLoad <- function(neededVector) {
for(thispackage in neededVector) {
if( ! require(thispackage, character.only = T) )
{ install.packages(thispackage)}
require(thispackage, character.only = T)
}
}
needed <- c('glmnet', 'randomForest', 'splines', 'boot', 'ISLR')
installIfAbsentAndLoad(needed)
#Set up a vector to store the test errors from 2 Models: A & B
test.errors = rep(0,2)
# Create dataframe
attach(OJ) #OJ dataframe from ISLR
OJ$Purchase <- as.factor(OJ$Purchase)
x=model.matrix(Purchase~.,OJ)[,-1]
y=OJ$Purchase
#(0) Modify the code so the training set is 75% and test set is 25% of the dataframe
set.seed(5082)
train = sample(1:nrow(x), nrow(x)*.75)
test = (-train)
x.train = x[train,]
x.test = x[-train, ]
y.train = y[train]
y.test =y[-train]
###################################################
####        Model B: Natural Spline            ####
###################################################
set.seed(5082)
library(splines)
#(2) Create a vector of 8 elements, filled with zeros
cv.vector <- rep(0,8)
#(3) Using the feature identified from step (1)
#    and the training dataset
#    Train a cubic Natural Spline model
#    with 1 to 8 knots
#    using a for loop
#    Store the cross-validation errors into the vector created in step (2)
trainDF <- data.frame(OJ[train,])
for (i in 4:11 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-3] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 3:11 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-3] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
for (i in 2:11 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-3] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
ns(LoyalCH, df = i)
ns(LoyalCH, df = 1)
ns(LoyalCH, df = 2)
ns(LoyalCH, df = 3)
ns(LoyalCH, df = 1)
ns(LoyalCH, df = 2)
ns(LoyalCH, df = 3)
ns(LoyalCH, df = 4)
ns(LoyalCH, df = 4)
ns(LoyalCH, df = 1)
ns(LoyalCH, df = 2)
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
for (i in 2:9 ){
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = i), data = trainDF,family = 'binomial')
cv.vector[i-1] <- cv.glm(trainDF, natty_spline, K=10)$delta[2]
x}
#(4) Plot the cross-validation errors by number of knots from step (3)
#    Plot the 1-SD line for model selection
#    Using the 1-SD rule, choose the best model
#    In a comment line, provide the best value for the tuning parameter
plot(cv.vector, xlab = 'Knots',ylab = 'CV errors')
stdev <- sd(cv.vector[1:8])
abline(h= min(cv.vector) + stdev, col = "red", lty = "dashed")
#(5) Make predictions using the best model from step (4)
#    Create a vetor of predicted probabilities
natty_spline <- glm(Purchase ~ ns(LoyalCH, df = 2), data = trainDF,
family = 'binomial')
testDF <- data.frame(LoyalCH = OJ[-train,]$LoyalCH)
ns.probs <- predict(natty_spline, testDF, type = 'response')
#(6) Convert probabilities into categorical decisions
#    if probability is greater than or equal to .5, then classify it as MM
#    otherwise classify it as CH
ns.pred <- ifelse(ns.probs > .5, "MM", "CH")
#(7) Display the confusion matrix
#    Compute and display the test error rate
#    Store the test error rate into the second element of test.errors
(table(ns.pred,y.test))
test.errors[2] <- mean(ns.pred != y.test)
which.min(test.errors)
?cv.glm
library(boot)
?cv.glm
?kmeans
?hclust
knitr::opts_chunk$set(echo = TRUE)
set.seed(2)
x=matrix(rnorm (50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
View(x)
set.seed(2)
x=matrix(rnorm (50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3   # Th
x=matrix(rnorm (50*2), ncol=2)
set.seed(2)
x=matrix(rnorm (50*2), ncol=2)
-0.8969+3
mean(x[1:25,1])
mean(x[25:50,1])
View(x)
rnorm
rnorm()
?rnorm
km.out=kmeans (x,2, nstart =20)
km.out$cluster
plot(x, col=(km.out$cluster +1),
main="K-Means Clustering Results with K=2",
xlab="", ylab="", pch=20, cex=2)
set.seed(4)
km.out <- kmeans (x,3, nstart =20)
km.out
set.seed(3)
km.out <- kmeans (x,3, nstart =1)
km.out$tot.withinss
km.out <- kmeans (x,3, nstart =20)
km.out$tot.withinss
set.seed(3)
km.out <- kmeans (x,3, nstart =1) # nstart: if centers is a number how
#         many random sets should be chosen?
km.out$tot.withinss
km.out <- kmeans (x,3, nstart =20)
km.out$tot.withinss
set.seed(3)
km.out <- kmeans (x,3, nstart =1) # nstart: if centers is a number how
#         many random sets should be chosen?
km.out$tot.withinss
km.out <- kmeans (x,3, nstart =20)
km.out$tot.withinss
km.out <- kmeans (x,3, nstart =20)
km.out$tot.withinss
km.out <- kmeans (x,3, nstart =20)
km.out$tot.withinss
set.seed(2)
x <- matrix(rnorm (50*2), ncol=2) # Create a 50 X 2 matrix with samples from a
# normal distribution
x[1:25,1] <- x[1:25,1]+3   # ask?>???
x[1:25,2] <- x[1:25,2]-4
xmean <- matrix(rnorm(4, sd = 4)2,2)
xmean <- matrix(rnorm(4, sd = 4),2,2)
whichOnes <- sample(1:2,50, replace = T)
xmean <- matrix(rnorm(4, sd = 4),2,2)
xmean <- matrix(rnorm(4, sd = 4),2,2)
xmean <- matrix(rnorm(4, sd = 4),2,2)
whichOnes <- sample(1:2,50, replace = T)
xmean <- matrix(rnorm(4, sd = 4),2,2)
whichOnes <- sample(1:2,50, replace = T)
xmean <- matrix(rnorm(4, sd = 4),4,2)
whichOnes <- sample(1:2,50, replace = T)
xmean <- matrix(rnorm(4, sd = 4),2,2)
whichOnes <- sample(3:4,50, replace = T)
x + xmean[whichOnes]
set.seed(2)
x <- matrix(rnorm (50*2), ncol=2) # Create a 50 X 2 matrix with samples from a
# normal distribution
x[1:25,1] <- x[1:25,1]+3   # ask?>???
x[1:25,2] <- x[1:25,2]-4
whichOnes <- sample(c(3,-4),50, replace = T)
whichOnes <- sample(c(3,-4),50, replace = T)
xmean <- matrix(rnorm(4, sd = 4),2,2)
whichOnes <- sample(c(3,-4),50, replace = T)
x + xmean[whichOnes]
xmean <- matrix(rnorm(4, sd = 4),2,2)
whichOnes <- sample(c(3,-4),50, replace = T)
x + xmean[whichOnes]
1
1
x + xmean[whichOnes]
hc.complete =hclust(dist(x), method="complete") # tells type of cluster method, type of distance used, and total data points
hc.average <- hclust(dist(x), method ="average")
hc.single <- hclust(dist(x), method ="single")
par(mfrow=c(1,3))
plot(hc.complete ,main="Complete Linkage ", xlab="", sub="",
cex=.9) # produces a cleaner plot that is easy to follow
plot(hc.average , main="Average Linkage", xlab="", sub="",
cex=.9)
plot(hc.single , main="Single Linkage", xlab="", sub="",
cex=.9) # produces branches for every single observation. Not the most comprehensive heirachrical plotting method
cutree(hc.complete , 2) # Shows complete separation of clusters classes
setwd("~/ML2/ML2.Clustering.io")
